{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 4: expected str instance, bytes found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-512a7425454d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtwokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/Codes/tweets_classification_edbt20/Steps/Step3/CNN/data_helpers/twokenize.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mdecorations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0membeddedApostrophe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0mHashtag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0;31m#AtMention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     ).decode('utf-8')), re.UNICODE)\n",
      "\u001b[0;32m~/Dropbox/Codes/tweets_classification_edbt20/Steps/Step3/CNN/data_helpers/twokenize.py\u001b[0m in \u001b[0;36mregex_or\u001b[0;34m(*items)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mregex_or\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m'(?:'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'|'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m')'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mContractions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu\"(?i)(\\w+)(n['’′]t|['’′]ve|['’′]ll|['’′]d|['’′]re|['’′]s|['’′]m)$\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUNICODE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: sequence item 4: expected str instance, bytes found"
     ]
    }
   ],
   "source": [
    "################################################################\n",
    "''' Preporcessing steps: \n",
    "1. lowercasing \n",
    "2. Digit -> DDD \n",
    "3. URLs -> httpAddress \n",
    "4. @username -> userID \n",
    "5. Remove special characters, keep ; . ! ? \n",
    "6. normalize elongation \n",
    "7. tokenization using tweetNLP\n",
    "output is ~/Dropbox (QCRI)/AIDR-DA-ALT-SC/data/labeled datasets/prccd_data/{filename}_AIDR_prccd.csv\n",
    "'''\n",
    "#################################################################\n",
    "\n",
    "#=================\n",
    "#==> Libraries <==\n",
    "#=================\n",
    "import re, os\n",
    "import string\n",
    "import sys\n",
    "\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "from os.path import basename\n",
    "import ntpath\n",
    "import codecs\n",
    "import unicodedata\n",
    "import optparse\n",
    "import logging\n",
    "import sys\n",
    "import csv\n",
    "import os\n",
    "import nltk\n",
    "#=============\n",
    "#==> Paths <==\n",
    "#=============\n",
    "prccd_folder = \"/test/india_floods.csv\" #no backslashes in front of special characters like spaces\n",
    "#prccd_folder = \"/Users/Imran/Dropbox/AIDR-DA-ALT-SC/sigir2016/data/out-domain/gold_silver/\"\n",
    "prccd_folder = os.path.expanduser(prccd_folder)\n",
    "\n",
    "#=================\n",
    "#==> Functions <==\n",
    "#=================\n",
    "#tweet = \"HIII! 1000 http://wwww.google.com @ALT :)\"\n",
    "\n",
    "def process(lst):\n",
    "    prccd_item_list=[]\n",
    "    for tweet in lst:\n",
    "        #\t\tprint \"[original]\", tweet\n",
    "        #                print(tweet)\n",
    "\n",
    "        # Normalizing utf8 formatting\n",
    "        tweet = tweet.decode(\"unicode-escape\").encode(\"utf8\").decode(\"utf8\")\n",
    "        #tweet = tweet.encode(\"utf-8\")\n",
    "        tweet = tweet.encode(\"ascii\",\"ignore\")\n",
    "        tweet = tweet.strip(' \\t\\n\\r')\n",
    "\n",
    "        # 1. Lowercasing\n",
    "        tweet = tweet.lower()\n",
    "        #\t\tprint \"[lowercase]\", tweet\n",
    "\n",
    "        # Word-Level\n",
    "        tweet = re.sub(' +',' ',tweet) # replace multiple spaces with a single space\n",
    "\n",
    "        # 2. Normalizing digits\n",
    "        tweet_words = tweet.strip('\\r').split(' ')\n",
    "        for word in [word for word in tweet_words if word.isdigit()]:\n",
    "            tweet = tweet.replace(word, \"D\" * len(word))\n",
    "        #\t\tprint \"[digits]\", tweet\n",
    "\n",
    "        # 3. Normalizing URLs\n",
    "        tweet_words = tweet.strip('\\r').split(' ')\n",
    "        for word in [word for word in tweet_words if '/' in word or '.' in word and  len(word) > 3]:\n",
    "            tweet = tweet.replace(word, \"httpAddress\")\n",
    "        #\t\tprint \"[URLs]\", tweet\n",
    "\n",
    "        # 4. Normalizing username\n",
    "        tweet_words = tweet.strip('\\r').split(' ')\n",
    "        for word in [word for word in tweet_words if word[0] == '@' and len(word) > 1]:\n",
    "            tweet = tweet.replace(word, \" \")\n",
    "        #\t\tprint \"[usrename]\", tweet\n",
    "\n",
    "        # 5. Removing special Characters\n",
    "        punc = '@$%^&*()_+-={}[]:\"|\\'\\~`<>/,'\n",
    "        trans = string.maketrans(punc, ' '*len(punc))\n",
    "        tweet = tweet.translate(trans)\n",
    "        #\t\tprint \"[punc]\", tweet\n",
    "\n",
    "        # 6. Normalizing +2 elongated char\n",
    "        tweet = re.sub(r\"(.)\\1\\1+\",r'\\1\\1', tweet.decode('utf-8'))\n",
    "        #\t\tprint \"[elong]\", tweet\n",
    "\n",
    "        # 7. tokenization using tweetNLP\n",
    "        tweet = ' '.join(nltk.word_tokenize(tweet))\n",
    "        #\t\tprint \"[token]\", tweet\n",
    "\n",
    "        #8. fix \\n char\n",
    "        tweet = tweet.replace('\\n', ' ')\n",
    "\n",
    "        prccd_item_list.append(tweet.strip())\n",
    "    #\t\tprint \"[processed]\", tweet.replace('\\n', ' ')\n",
    "    return prccd_item_list\n",
    "\n",
    "#=====================\n",
    "#==> Main Function <==\n",
    "#=====================\n",
    "\n",
    "def main():\n",
    "\tcolumns = defaultdict(list) # each value in each column is appended to a list\n",
    "\n",
    "\twith open(\"/home/raj/mntShare/share/deep-learning-for-big-crisis-data-master/data/prccd_folder/india_floods.csv\", 'rU') as f:\n",
    "\t#with codecs.open(csvfile, \"r\", \"utf-8\") as f:\n",
    "\t\treader = csv.DictReader(f) # read rows into a dictionary format\n",
    "\t\tfor row in reader: # read a row as {column1: value1, column2: value2,...}\n",
    "\t\t\tfor (k,v) in row.items(): # go over each column name and value\n",
    "\t\t\t\tcolumns[k.strip()].append(v) # append the value into the appropriate list based on column name k\n",
    "\n",
    "\tprccd_item_list=process(columns['item'])\n",
    "\tname = os.path.splitext(\"/home/raj/mntShare/share/deep-learning-for-big-crisis-data-master/data/prccd_folder/india_floods.csv\")[0]\n",
    "#\tif not os.path.exists(prccd_folder): os.mkdir(prccd_folder, 0755)\n",
    "#        with open(prccd_folder+\"/%s_AIDR_prccd.csv\" % name, 'wb') as f:\n",
    "\twith open(\"%s_prccd.csv\" % name, 'wb') as f:\n",
    "\t#with codecs.open(\"%s_AIDR_prccd.csv\" % name, 'wb', \"utf-8\") as f:\n",
    "\t\twriter = csv.writer(f)\n",
    "\t\twriter.writerow([\"item_id\",\"item\",\"label\"])\n",
    "\t\trows = zip(columns['item_id'],prccd_item_list,columns['label'])\n",
    "#\t\trows = zip(prccd_item_list)\n",
    "\t\tfor row in rows:\n",
    "\t\t\twriter.writerow(row)\n",
    "\n",
    "#===========\n",
    "#==> Run <==\n",
    "#===========\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
