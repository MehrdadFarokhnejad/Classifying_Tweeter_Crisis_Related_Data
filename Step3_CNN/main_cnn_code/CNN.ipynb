{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "''' python 3.7\n",
    "Train LSTM RNNs on the AIDR tweet classification task.\n",
    "\n",
    "GPU command:\n",
    "    THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python lstm_rnns_aidr.py\n",
    "\n",
    "Output after 4 epochs on CPU: ~0.8146\n",
    "Time per epoch on CPU (Core i7): ~150s.\n",
    "'''\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(437)  # for reproducibility\n",
    "\n",
    "# keras related\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core    import Dense, Dropout, Activation, Flatten\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "from utilities import aidr\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from keras.models import model_from_json\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, BatchNormalization\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import adam\n",
    "import warnings\n",
    "#other utilities\n",
    "import optparse\n",
    "import logging\n",
    "import sys\n",
    "import csv\n",
    "import os\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "from sklearn import metrics\n",
    "import warnings\n",
    "\n",
    "def build_cnn(maxlen, max_features, emb_size=128, emb_matrix=None, nb_filter=250, filter_length=3,\n",
    "            pool_length=2,nb_classes = 2,  hidden_size=128, dropout_ratio=0.5, tune_emb=True):\n",
    "\n",
    "    ''' build cnn model '''\n",
    "\n",
    "    print('Building model:', 'convolutional neural network (cnn)')\n",
    "\n",
    "    #create the emb layer\n",
    "    if emb_matrix is not None:\n",
    "        max_features, emb_size = emb_matrix.shape\n",
    "        emb_layer = Embedding(max_features, emb_size, weights=[emb_matrix], input_length=maxlen, trainable=tune_emb)\n",
    "\n",
    "    else:\n",
    "        emb_layer = Embedding(max_features, emb_size, input_length=maxlen, trainable=tune_emb)\n",
    "\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(emb_layer)\n",
    "    model.add(Dropout(dropout_ratio))\n",
    "\n",
    "    # we add a Convolution1D, which will learn nb_filter (word group) filters of size filter_length:\n",
    "    model.add(Convolution1D(nb_filter=nb_filter, filter_length=filter_length,\n",
    "                            border_mode='valid', activation='relu', subsample_length=1))\n",
    "\n",
    "    # we use standard max pooling (halving the output of the previous layer):\n",
    "    model.add(MaxPooling1D(pool_length=pool_length))\n",
    "    model.add(Dropout(dropout_ratio))\n",
    "\n",
    "    # We flatten the output of the conv layer, so that we can add a vanilla dense layer:\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # We add a vanilla hidden layer:\n",
    "    model.add(Dense(hidden_size))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(dropout_ratio))\n",
    "\n",
    "    # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "\n",
    "    if nb_classes == 2:\n",
    "        print('Doing binary classification...')\n",
    "        model.add(Dense(1))\n",
    "        model.add(Activation('sigmoid'))\n",
    "\n",
    "    elif nb_classes > 2:\n",
    "        print('Doing classification with class #', nb_classes)\n",
    "        model.add(Dense(nb_classes))\n",
    "        model.add(Activation('softmax'))\n",
    "    else:\n",
    "        print(\"Wrong argument nb_classes: \", nb_classes)\n",
    "        exit(1)\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper-parameters Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s %(levelname)s: %(message)s', datefmt='%Y-%m-%d %H:%M:%S', level=logging.DEBUG)\n",
    "\n",
    "# parse user input\n",
    "parser = optparse.OptionParser(\"%prog [options]\")\n",
    "\n",
    "#file related options\n",
    "parser.add_option(\"-g\", \"--log-file\",          dest=\"log_file\", help=\"log file [default: %default]\")\n",
    "parser.add_option(\"-d\", \"--data-dir\",          dest=\"data_dir\", help=\"directory containing train, test and dev file [default: %default]\")\n",
    "parser.add_option(\"-D\", \"--data-spec\",         dest=\"data_spec\", help=\"specification for training data (in, out, in_out) [default: %default]\")\n",
    "parser.add_option(\"-m\", \"--model-dir\",         dest=\"model_dir\", help=\"directory to save the best models [default: %default]\")\n",
    "\n",
    "# network related\n",
    "parser.add_option(\"-t\", \"--max-tweet-length\",  dest=\"maxlen\",       type=\"int\", help=\"maximul tweet length (for fixed size input) [default: %default]\") # input size\n",
    "\n",
    "parser.add_option(\"-F\", \"--nb_filter\",         dest=\"nb_filter\",     type=\"int\",   help=\"nb of filter to be applied in convolution over words [default: %default]\") # uni, bi-directional\n",
    "parser.add_option(\"-r\", \"--filter_length\",     dest=\"filter_length\", type=\"int\",   help=\"length of neighborhood in words [default: %default]\") # lstm, gru, simpleRNN\n",
    "parser.add_option(\"-p\", \"--pool_length\",       dest=\"pool_length\",   type=\"int\",   help=\"length for max pooling [default: %default]\") # lstm, gru, simpleRNN\n",
    "parser.add_option(\"-v\", \"--vocabulary-size\",   dest=\"max_features\",  type=\"float\",   help=\"vocabulary size in percentage [default: %default]\") # emb matrix row size\n",
    "parser.add_option(\"-e\", \"--emb-size\",          dest=\"emb_size\",      type=\"int\",   help=\"dimension of embedding [default: %default]\") # emb matrix col size\n",
    "parser.add_option(\"-s\", \"--hidden-size\",       dest=\"hidden_size\",   type=\"int\",   help=\"hidden layer size [default: %default]\") # size of the hidden layer\n",
    "parser.add_option(\"-o\", \"--dropout_ratio\",     dest=\"dropout_ratio\", type=\"float\", help=\"ratio of cells to drop out [default: %default]\")\n",
    "\n",
    "parser.add_option(\"-i\", \"--init-type\",         dest=\"init_type\",     help=\"random or pretrained [default: %default]\")\n",
    "parser.add_option(\"-f\", \"--emb-file\",          dest=\"emb_file\",      help=\"file containing the word vectors [default: %default]\")\n",
    "parser.add_option(\"-P\", \"--tune-emb\",          dest=\"tune_emb\",      action=\"store_false\", help=\"DON't tune word embeddings [default: %default]\")\n",
    "\n",
    "    #learning related\n",
    "parser.add_option(\"-a\", \"--learning-algorithm\", dest=\"learn_alg\", help=\"optimization algorithm (adam, sgd, adagrad, rmsprop, adadelta) [default: %default]\")\n",
    "parser.add_option(\"-b\", \"--minibatch-size\",     dest=\"minibatch_size\", type=\"int\", help=\"minibatch size [default: %default]\")\n",
    "parser.add_option(\"-l\", \"--loss\",               dest=\"loss\", help=\"loss type (hinge, squared_hinge, binary_crossentropy) [default: %default]\")\n",
    "parser.add_option(\"-n\", \"--epochs\",             dest=\"epochs\", type=\"int\", help=\"nb of epochs [default: %default]\")\n",
    "\n",
    "\n",
    "parser.set_defaults(\n",
    "    data_dir        = \"../data/\"\n",
    "    ,data_spec       = \"in\"\n",
    "    ,log_file       = \"log\"\n",
    "    ,model_dir      = \"../saved_models/\"\n",
    "    ,featFile_train = \"../data/sample-prccd_train.csv\"\n",
    "    ,featFile_test  = \"../data/sample-prccd_test.csv\"\n",
    "    ,featFile_dev   = \"../data/sample-prccd_dev.csv\"\n",
    "\n",
    "    ,learn_alg      = \"adadelta\" # sgd, adagrad, rmsprop, adadelta, adam (default)\n",
    "    ,loss           = \"binary_crossentropy\" # hinge, squared_hinge, binary_crossentropy (default)\n",
    "    ,minibatch_size = 32\n",
    "    ,dropout_ratio  = 0.0\n",
    "    ,maxlen         = 100\n",
    "    ,epochs         = 25\n",
    "    ,max_features   = 80\n",
    "    ,emb_size       = 128\n",
    "    ,hidden_size    = 128\n",
    "    ,nb_filter      = 250\n",
    "    ,filter_length  = 3\n",
    "    ,pool_length    = 2\n",
    "    ,init_type      = 'random'\n",
    "    ,emb_file       = \"../embeddings/crisis_embeddings.txt/\"\n",
    "    ,tune_emb       = True\n",
    ")\n",
    "\n",
    "options,args = parser.parse_args(sys.argv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the model into Train and Test Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Reading vocabulary from../data/sample_prccd_train.csv\n",
      "okay\n",
      "Reading vocabulary from../data/sample_prccd_test.csv\n",
      "okay\n",
      "Reading vocabulary from../data/sample_prccd_dev.csv\n",
      "okay\n",
      "Nb of tweets: train: 834 test: 237 dev: 118\n",
      "Total vocabulary size: 1842\n",
      "Pruned vocabulary size: 80% =1473\n",
      "Random seed 113\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "print('Loading data...')\n",
    "(X_train, y_train), (X_test, y_test), (X_dev, y_dev), max_features, E, label_id = aidr.load_and_numberize_data(path=options.data_dir,\n",
    "                                                                            nb_words=options.max_features, init_type=options.init_type,\n",
    "                                                                            embfile=options.emb_file, dev_train_merge=0, map_labels_to_five_class=0)\n",
    "print('test')\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=options.maxlen)\n",
    "X_test  = sequence.pad_sequences(X_test,  maxlen=options.maxlen)\n",
    "X_dev   = sequence.pad_sequences(X_dev,   maxlen=options.maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building CNN model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............................\n",
      "834 train tweets\n",
      "237 test  tweets\n",
      "118 dev   tweets\n",
      "1473 vocabulary size\n",
      "2 different classes\n",
      "............................\n",
      "Building model: convolutional neural network (cnn)\n",
      "Doing binary classification...\n",
      "../saved_models/cnn-adadelta-250-3-2-True-binary_crossentropy-32-0.0-init-random-80-128-128.model.cl.2.dom.in\n",
      "yes\n",
      "Training and validating ....\n",
      "fitting\n",
      "Train on 834 samples, validate on 118 samples\n",
      "Epoch 1/25\n",
      "834/834 [==============================] - 5s 6ms/step - loss: 0.3594 - accuracy: 0.8801 - val_loss: 0.3939 - val_accuracy: 0.8983\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.39390, saving model to ../saved_models/cnn-adadelta-250-3-2-True-binary_crossentropy-32-0.0-init-random-80-128-128.model.cl.2.dom.in\n",
      "Epoch 2/25\n",
      "834/834 [==============================] - 4s 5ms/step - loss: 0.3244 - accuracy: 0.8969 - val_loss: 0.3381 - val_accuracy: 0.8983\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.39390 to 0.33810, saving model to ../saved_models/cnn-adadelta-250-3-2-True-binary_crossentropy-32-0.0-init-random-80-128-128.model.cl.2.dom.in\n",
      "Epoch 3/25\n",
      "834/834 [==============================] - 4s 5ms/step - loss: 0.2738 - accuracy: 0.8969 - val_loss: 0.3133 - val_accuracy: 0.8983\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.33810 to 0.31331, saving model to ../saved_models/cnn-adadelta-250-3-2-True-binary_crossentropy-32-0.0-init-random-80-128-128.model.cl.2.dom.in\n",
      "Epoch 4/25\n",
      "834/834 [==============================] - 5s 5ms/step - loss: 0.1985 - accuracy: 0.9053 - val_loss: 0.2075 - val_accuracy: 0.9068\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.31331 to 0.20747, saving model to ../saved_models/cnn-adadelta-250-3-2-True-binary_crossentropy-32-0.0-init-random-80-128-128.model.cl.2.dom.in\n",
      "Epoch 5/25\n",
      "834/834 [==============================] - 5s 5ms/step - loss: 0.1575 - accuracy: 0.9472 - val_loss: 0.1840 - val_accuracy: 0.9153\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.20747 to 0.18404, saving model to ../saved_models/cnn-adadelta-250-3-2-True-binary_crossentropy-32-0.0-init-random-80-128-128.model.cl.2.dom.in\n",
      "Epoch 6/25\n",
      "834/834 [==============================] - 5s 6ms/step - loss: 0.1167 - accuracy: 0.9592 - val_loss: 0.1719 - val_accuracy: 0.9153\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.18404 to 0.17195, saving model to ../saved_models/cnn-adadelta-250-3-2-True-binary_crossentropy-32-0.0-init-random-80-128-128.model.cl.2.dom.in\n",
      "Epoch 7/25\n",
      "834/834 [==============================] - 5s 6ms/step - loss: 0.0931 - accuracy: 0.9736 - val_loss: 0.1993 - val_accuracy: 0.9153\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.17195\n",
      "Epoch 8/25\n",
      "834/834 [==============================] - 5s 6ms/step - loss: 0.0775 - accuracy: 0.9712 - val_loss: 0.2005 - val_accuracy: 0.9153\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.17195\n",
      "Epoch 9/25\n",
      "834/834 [==============================] - 5s 6ms/step - loss: 0.0456 - accuracy: 0.9832 - val_loss: 0.1872 - val_accuracy: 0.9237\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.17195\n",
      "Epoch 00009: early stopping\n",
      "Loading ... ../saved_models/cnn-adadelta-250-3-2-True-binary_crossentropy-32-0.0-init-random-80-128-128.model.cl.2.dom.in\n",
      "237/237 [==============================] - 1s 2ms/step\n",
      "Test accuracy: 0.949367105960846\n",
      "Predictions\n",
      "[0.5380514]\n",
      "[0.00741771]\n",
      "[0.02577615]\n",
      "[0.5141634]\n",
      "[0.00131842]\n",
      "[0.03198171]\n",
      "[0.00019595]\n",
      "[0.04152274]\n",
      "[0.33287525]\n",
      "[0.00049737]\n",
      "[0.8628117]\n",
      "[0.10328105]\n",
      "[0.00075504]\n",
      "[0.04444724]\n",
      "[0.01575518]\n",
      "[0.00282952]\n",
      "[0.01370654]\n",
      "[0.84652686]\n",
      "[0.0495187]\n",
      "[0.00525916]\n",
      "[0.80640304]\n",
      "[0.00908819]\n",
      "[0.00084028]\n",
      "[0.02533033]\n",
      "[0.00762215]\n",
      "[0.01628855]\n",
      "[0.0020442]\n",
      "[0.00066638]\n",
      "[0.84912205]\n",
      "[0.00080466]\n",
      "[5.2839518e-05]\n",
      "[0.02830255]\n",
      "[0.00304404]\n",
      "[0.09403285]\n",
      "[0.01720837]\n",
      "[0.02877355]\n",
      "[0.00242031]\n",
      "[0.05365095]\n",
      "[0.00071222]\n",
      "[0.00060943]\n",
      "[0.00070202]\n",
      "[0.05629396]\n",
      "[0.01600373]\n",
      "[0.00244391]\n",
      "[0.00092217]\n",
      "[0.00287804]\n",
      "[0.5285881]\n",
      "[0.00953084]\n",
      "[0.00236043]\n",
      "[0.10169351]\n",
      "[0.20462593]\n",
      "[0.1965062]\n",
      "[0.00228769]\n",
      "[0.00144458]\n",
      "[0.00906101]\n",
      "[0.00224733]\n",
      "[0.01011217]\n",
      "[0.06928667]\n",
      "[0.32981303]\n",
      "[0.00332212]\n",
      "[0.00021327]\n",
      "[0.00256041]\n",
      "[0.01247203]\n",
      "[0.00206238]\n",
      "[0.0002735]\n",
      "[0.0062989]\n",
      "[0.00435552]\n",
      "[0.00161439]\n",
      "[0.61885124]\n",
      "[0.00054052]\n",
      "[0.0001201]\n",
      "[0.00036949]\n",
      "[0.00053042]\n",
      "[0.00040308]\n",
      "[0.00841224]\n",
      "[0.00203624]\n",
      "[0.0077382]\n",
      "[0.00036949]\n",
      "[0.03588685]\n",
      "[0.01043579]\n",
      "[0.0003351]\n",
      "[0.00102469]\n",
      "[0.00061351]\n",
      "[0.00091082]\n",
      "[0.00026649]\n",
      "[0.00089732]\n",
      "[0.00096074]\n",
      "[0.00058961]\n",
      "[0.00011629]\n",
      "[0.00381342]\n",
      "[0.00014552]\n",
      "[0.8230711]\n",
      "[0.06818765]\n",
      "[0.00109544]\n",
      "[0.01503152]\n",
      "[0.01334679]\n",
      "[0.00829956]\n",
      "[0.00188705]\n",
      "[0.00083193]\n",
      "[0.17063034]\n",
      "[0.00643951]\n",
      "[0.2850836]\n",
      "[0.8613102]\n",
      "[3.284216e-05]\n",
      "[0.00216648]\n",
      "[0.441095]\n",
      "[0.02101278]\n",
      "[0.07703313]\n",
      "[0.1079675]\n",
      "[0.00840974]\n",
      "[0.4503142]\n",
      "[0.00471762]\n",
      "[0.01542524]\n",
      "[0.0006763]\n",
      "[0.09212959]\n",
      "[0.00075504]\n",
      "[0.00234124]\n",
      "[0.01255822]\n",
      "[0.00657162]\n",
      "[0.04278311]\n",
      "[0.00081649]\n",
      "[0.12866873]\n",
      "[0.00075504]\n",
      "[0.6928284]\n",
      "[0.00582898]\n",
      "[0.00284863]\n",
      "[0.03375089]\n",
      "[0.00059777]\n",
      "[0.00014272]\n",
      "[0.02302161]\n",
      "[0.00079331]\n",
      "[0.00425491]\n",
      "[0.00056797]\n",
      "[0.06647074]\n",
      "[0.0011948]\n",
      "[0.66287994]\n",
      "[0.00099385]\n",
      "[0.00047046]\n",
      "[0.00450569]\n",
      "[0.00050583]\n",
      "[0.00053868]\n",
      "[0.00033718]\n",
      "[0.01078919]\n",
      "[0.5460731]\n",
      "[0.01377708]\n",
      "[0.00471196]\n",
      "[0.00244144]\n",
      "[0.00753489]\n",
      "[0.00037122]\n",
      "[0.23917651]\n",
      "[0.00404406]\n",
      "[0.00081173]\n",
      "[0.06443667]\n",
      "[0.00011367]\n",
      "[0.0486035]\n",
      "[0.00657162]\n",
      "[0.0154531]\n",
      "[0.00085035]\n",
      "[0.00164673]\n",
      "[0.00980237]\n",
      "[0.66368747]\n",
      "[0.00250682]\n",
      "[0.04177666]\n",
      "[0.34993494]\n",
      "[0.10504183]\n",
      "[0.0018084]\n",
      "[0.7618965]\n",
      "[0.01671642]\n",
      "[0.00152361]\n",
      "[0.00333509]\n",
      "[0.6091219]\n",
      "[0.00133723]\n",
      "[0.00479156]\n",
      "[0.01667878]\n",
      "[0.00010973]\n",
      "[0.08515143]\n",
      "[0.0004701]\n",
      "[0.23753262]\n",
      "[0.00093898]\n",
      "[0.0004738]\n",
      "[0.01501229]\n",
      "[0.8537029]\n",
      "[0.27810782]\n",
      "[0.46922913]\n",
      "[0.00033858]\n",
      "[7.6413155e-05]\n",
      "[0.12722114]\n",
      "[0.00314564]\n",
      "[0.0001193]\n",
      "[0.00317016]\n",
      "[0.00061017]\n",
      "[0.0005033]\n",
      "[0.00055128]\n",
      "[0.12915257]\n",
      "[0.00040221]\n",
      "[0.18159714]\n",
      "[0.00026694]\n",
      "[0.00546178]\n",
      "[0.0073292]\n",
      "[0.00133187]\n",
      "[0.01247203]\n",
      "[0.01141033]\n",
      "[0.00087512]\n",
      "[0.00180069]\n",
      "[0.09871176]\n",
      "[0.11434171]\n",
      "[0.09501451]\n",
      "[0.00082797]\n",
      "[0.00224733]\n",
      "[0.19015211]\n",
      "[0.00826418]\n",
      "[0.00085616]\n",
      "[0.00687003]\n",
      "[0.09041113]\n",
      "[0.17519104]\n",
      "[0.28157938]\n",
      "[0.00224733]\n",
      "[0.00329012]\n",
      "[0.00190741]\n",
      "[0.04553595]\n",
      "[0.00406262]\n",
      "[0.5436181]\n",
      "[0.8159641]\n",
      "[0.01692557]\n",
      "[0.03466314]\n",
      "[0.07622454]\n",
      "[0.00304648]\n",
      "[0.01247203]\n",
      "[0.00252649]\n",
      "[0.01139516]\n",
      "[0.01240629]\n",
      "[0.0086509]\n",
      "[0.00048789]\n",
      "[0.00669175]\n",
      "[0.3534103]\n",
      "[0.5900816]\n",
      "[0.1556112]\n",
      "ROC Prediction (binary classification): 0.9616588419405321\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "nb_classes = np.max(y_train) + 1\n",
    "\n",
    "print('............................')\n",
    "print(len(X_train), 'train tweets')\n",
    "print(len(X_test),  'test  tweets')\n",
    "print(len(X_dev),   'dev   tweets')\n",
    "print(max_features - 3, 'vocabulary size')\n",
    "print(nb_classes, 'different classes')\n",
    "print('............................')\n",
    "\n",
    "\n",
    "if nb_classes == 2: # binary\n",
    "    loss       = options.loss\n",
    "    class_mode = \"binary\"\n",
    "    optimizer  = options.learn_alg\n",
    "\n",
    "elif nb_classes > 2: # multi-class\n",
    "    loss       = 'categorical_crossentropy'\n",
    "    class_mode = 'categorical'\n",
    "    optimizer  = options.learn_alg\n",
    "    print(\"** optimizer: \" + options.learn_alg)\n",
    "    # convert class vectors to binary class matrices [ 1 of K encoding]\n",
    "    y_train_mod = np_utils.to_categorical(y_train, nb_classes)\n",
    "    y_test_mod  = np_utils.to_categorical(y_test,  nb_classes)\n",
    "    y_dev_mod   = np_utils.to_categorical(y_dev,   nb_classes)\n",
    "\n",
    "\n",
    "model = build_cnn(options.maxlen, max_features, emb_matrix=E, emb_size=options.emb_size, nb_filter=options.nb_filter,\n",
    "                    filter_length=options.filter_length, pool_length=options.pool_length, nb_classes = nb_classes,\n",
    "                    hidden_size=options.hidden_size, dropout_ratio=options.dropout_ratio, tune_emb=options.tune_emb)\n",
    "    \n",
    "    #model.compile(optimizer=optimizer, loss=loss,  class_mode=class_mode)\n",
    "\n",
    "model.compile(optimizer=optimizer , loss=loss, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_name = options.model_dir + \"cnn\" + \"-\" + optimizer + \"-\" + str(options.nb_filter) + \"-\" + str(options.filter_length) + \\\n",
    "    \"-\" + str(options.pool_length) + \"-\" + str (options.tune_emb) +\\\n",
    "    \"-\" + loss + \"-\" + str (options.minibatch_size) + \"-\" + str(options.dropout_ratio) + \"-init-\" + str (options.init_type) + \"-\" +\\\n",
    "    str (options.max_features) + \"-\" + str (options.emb_size) + \"-\" + str (options.hidden_size) + \".model.cl.\" + str(nb_classes) + \".dom.\" + str(options.data_spec)\n",
    "    \n",
    "    \n",
    "print(model_name)\n",
    "    \n",
    "    \n",
    "earlystopper = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
    "checkpointer = ModelCheckpoint(filepath=model_name, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "\n",
    "print('yes')\n",
    "if nb_classes == 2: # binary\n",
    "    print ('Training and validating ....')\n",
    "    print(\"fitting\")\n",
    "    model.fit(X_train, y_train, verbose=1,batch_size=options.minibatch_size, nb_epoch=options.epochs , validation_data=(X_dev, y_dev),callbacks=[earlystopper, checkpointer])\n",
    "    #model.fit(X_train, y_train, batch_size=options.minibatch_size, nb_epoch=options.epochs,\n",
    "            #validation_data=(X_dev, y_dev), show_accuracy=True, verbose=2, callbacks=[earlystopper, checkpointer])\n",
    "\n",
    "    print (\"Loading ...\", model_name)\n",
    "    model.load_weights(model_name)\n",
    "    score, acc = model.evaluate(X_test, y_test, batch_size=options.minibatch_size)\n",
    "    print('Test accuracy:', acc)\n",
    "    y_prob = model.predict_proba(X_test)\n",
    "    ##added by kamla\n",
    "    print(\"Predictions\")\n",
    "    for e in y_prob: print(e)\n",
    "    ###\n",
    "    roc = metrics.roc_auc_score(y_test, y_prob)\n",
    "    print(\"ROC Prediction (binary classification):\", roc)\n",
    "\n",
    "\n",
    "elif nb_classes > 2: # multi-class\n",
    "    print ('Training and validating ....')\n",
    "    model.fit(X_train, y_train_mod, batch_size=options.minibatch_size, nb_epoch=options.epochs,\n",
    "                validation_data=(X_dev, y_dev_mod), verbose=2,\n",
    "                callbacks=[earlystopper, checkpointer])\n",
    "    print (\"Loading ...\", model_name)\n",
    "    model.load_weights(model_name)\n",
    "    print(\"Test model2 ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_Raw Accuracy: 0.9493670886075949\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  __label__1     0.9631    0.9812    0.9721       213\n",
      "  __label__2     0.8000    0.6667    0.7273        24\n",
      "\n",
      "    accuracy                         0.9494       237\n",
      "   macro avg     0.8816    0.8239    0.8497       237\n",
      "weighted avg     0.9466    0.9494    0.9473       237\n",
      "\n",
      "CNN_Confusion Matrix:\n",
      " [[209   4]\n",
      " [  8  16]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_pred = model.predict_classes(X_test)\n",
    "y_test = np.array(y_test)\n",
    "acc2 = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"CNN_Raw Accuracy:\", acc2)\n",
    "#get label ids in sorted\n",
    "class_labels = sorted(label_id, key=label_id.get)\n",
    "#print (class_labels)\n",
    "\n",
    "print (metrics.classification_report(y_test, y_pred, target_names=class_labels, digits=4) )\n",
    "print (\"CNN_Confusion Matrix:\\n\", metrics.confusion_matrix(y_test, y_pred, labels=range(0, len(class_labels))))\n",
    "\n",
    "if nb_classes == 2:\n",
    "    _p, _r, _f, sup = metrics.precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
    "    # print (\" pre: \" + str (_p) + \" rec: \" + str (_r) + \" f-score: \" + str (_f))\n",
    "\n",
    "else:\n",
    "    mic_p, mic_r, mic_f, sup = metrics.precision_recall_fscore_support(y_test, y_pred, average='micro')\n",
    "    mac_p, mac_r, mac_f, sup = metrics.precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "    print (\" micro pre: \" + str (mic_p) + \" rec: \" + str (mic_r) + \" f-score: \" + str (mic_f))\n",
    "    print (\" macro pre: \" + str (mac_p) + \" rec: \" + str (mac_r) + \" f-score: \" + str (mac_f))\n",
    "    # save the architecture finally in json format\n",
    "    json_string = model.to_json()\n",
    "    open(model_name + \".json\", 'w').write(json_string)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
